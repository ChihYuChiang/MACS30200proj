---
title: "PS#3: Hodgepodge"
author: |
  | Chih-Yu Chiang
  | UCID 12145146
date: "May 14, 2017"
output: github_document
---
```{r setup, include=FALSE}
library(tidyverse)
library(modelr)
library(knitr)
library(broom)
library(pander)
library(lmtest)
library(car)
library(coefplot)
library(plotly)
library(stringr)
library(haven)
library(GGally)
library(Amelia)
library(forcats)
library(car)
library(rcfss)
library(RColorBrewer)

options(digits=4)

```




## Part 1: Regression diagnostics
### Estimate model
```{r Estimate model P1}
df <- read_csv("data/biden.csv") %>%
  na.omit() %>%
  rownames_to_column(var="row")

lm_1 <- lm(biden ~ age + female + educ, data=df)
pander(summary(lm_1))

```
  
After applying listwise missing value deletion, the above model is estimated with 1,807 observations, `age`, `female` (gender), and `educ` (education) as predictor variables, and `biden` (Joe Biden feeling thermometer) as the outcome variable.  
  
Estimated parameters, standard errors, and significance are reported in the first table.  
  

### 1. Test the model to identify any unusual and/or influential observations. Identify how you would treat these observations moving forward with this research.
```{r P1-1 Basic}
pander(summary(df))

ggplot(df) +
  geom_histogram(aes(x=biden), binwidth=1) +
  labs(title="Histogram of Joe Biden feeling thermometer",
       x="Joe Biden feeling thermometer",
       y="Observation count")

ggplot(df) +
  geom_histogram(aes(x=educ), binwidth=1) +
  labs(title="Histogram of education",
       x="Education",
       y="Observation count")

ggplot(df) +
  geom_histogram(aes(x=age), binwidth=1) +
  labs(title="Histogram of age",
       x="Age",
       y="Observation count")

```
  
To observe the unusual observations, first, I examine descriptive statistics and histograms of variables. From the descriptive summary, we make sure that all the binary variables, `female`, `dem`, and `rep`, contain only appropriate responses (0 or 1). I then turn to the histograms to examine the continuous (or order) variables. From the graphs, we can see some observations with `biden` at 0, some with `educ` at lower level, such as less than 7, and some with `age`, such as higher than 80--they stray from the major part of the observations and could potentially have disproportionate influences on our model. To check if their influence a problem of our model, next, I do a bubble plot with influential indicators.  

```{r P1-1 Bubble}
#Add key statistics
df_augment <- df %>%
  mutate(hat=hatvalues(lm_1),
         student=rstudent(lm_1),
         cooksd=cooks.distance(lm_1))

# draw bubble plot
ggplot(df_augment, aes(hat, student)) +
  geom_hline(yintercept=0, linetype=2) +
  geom_point(aes(size=cooksd), shape=1) +
  geom_text(data=df_augment %>%
              arrange(-cooksd) %>%
              slice(1:10),
            aes(label=row, color="orange")) +
  scale_size_continuous(range=c(1, 20)) +
  labs(title="Bubble plot of influential indicators",
       subtitle="Leverage, Sutentized residual, and Cook’s D (bubble size)",
       x="Leverage",
       y="Studentized residual") +
  theme(legend.position ="none")

```
  
The bubble size denotes the observation's Cook's D value. In the graph, I identify the ten observations with the highest Cook's D value, with their IDs marked in the bubbles, including observation 408, 609, 1086, 1682, etc.  

```{r P1-1 Stats}
hat <- df_augment %>%
  filter(hat > 2 * mean(hat))

student <- df_augment %>%
  filter(abs(student) > 2)

cooksd <- df_augment %>%
  filter(cooksd > 4 / (nrow(.) - (length(coef(lm_1)) - 1) - 1))

bind_rows(hat, student, cooksd)

```
  
I then do some quick rules of thumb stat examination, including standards for Hat-values, Studentized residuals, and Cook's D indicator:  

* Anything with Hat-values exceeding twice the average Hat-values.  
* Anything with Studentized residuals outside of the range [−2,2].  
* Anything with Cook's D > 4/(n-k-1), where n is the number of observations and k is the number of coefficients in the regression model.
  
I filter to acquire the 246 observations who fall into any of the above three groups. In general, the identified observations have either `biden` = 0, lower `educ` level, or/and higher `age` level. This result conforms to what we concluded in observing the data summary and histograms.  
  
While those observations are identified visually and statistically, they are all still reasonable for me. Their values still fall in reasonable range. For example, people are possible to have a low level of biden affection to have `biden` = 0; it is also not uncommon to have respondants in their 80s or 90s, or with fewer years of education. In other words, these identified observations are probably not caused by survey or recording mistakes. The reason why they are identified here is probably just because, naturally in the population, fewer people have those traits. For example, it is natural that there are less people in their 90s than in 40s or 50s. Accordingly, potential treatments in dealing with these observations are proposed as follows:  

1. As these observations seem not unreasonable and could represent distribution of the real population, it is less proper to just delete them listwise--that could make the model less representative to the real situation.
2. Instead, I would suggest keep these observations in estimating the model. Perhaps, we should also consider collecting more data with those identified traits to estimate the traits at these identified levels better.
3. Another action can be taken is that redoing this survey with extended scale for the survey item evaluating biden thermometer. While many respondants answered `biden` at level 0 and 100, this could indicate the scale is not complete enough to account for the diverse level of biden affection. For example, if the upper limit is extended to 200, the observations can be scattered into a larger range, which leads to a larger standard error and could make these "unusual" observations normal (they are normal by nature, in some sense).  


### 2. Test for non-normally distributed errors. If they are not normally distributed, propose how to correct for them.
```{r P1-2 QQ and dis}
car::qqPlot(lm_1)

augment(lm_1, df) %>%
  mutate(.student=rstudent(lm_1)) %>%
  ggplot(aes(.student)) +
  geom_density(adjust=.5) +
  labs(title = "Density plot of the studentized residuals",
       x="Studentized residuals",
       y="Estimated density")

```
  
I apply the qqplot and the density plot of the studentized residuals to observe if there are the non-normally distributed errors. In the qqplot, many of the observation points stray outside the 95% confidence intervals marked by the dashed red lines at higher levels (> 1.5) and lower levels (at around -2) of t quantiles. This is confirmed in the residual plot, where the distribution is apparently left skewed, with too many observations having higher level residuals. These evidences indicate the extant of non-normally distributed problem.  

```{r P1-2 Solution}
df_power <- df %>%
  mutate(biden_power = biden**1.4)

lm_1_2 <- lm(biden_power ~ age + female + educ, data=df_power)
pander(summary(lm_1_2))

car::qqPlot(lm_1_2)

augment(lm_1_2, df_power) %>%
  mutate(.student = rstudent(lm_1_2)) %>%
  ggplot(aes(.student)) +
  geom_density(adjust = .5) +
  labs(title = "Density plot of the studentized residuals",
       x = "Studentized residuals",
       y = "Estimated density")

```
  
To correct the non-normally distributed error, I propose a power transformation for the `biden` variable. I try transforming `biden` to the power of 1.4; the result is displayed as above. In qqplot, at the identifed levels, substantially less observations fall outside the confidence interval. The residual distribution is also less skewed compared to the original one.  


### 3. Test for heteroscedasticity in the model. If present, explain what impact this could have on inference.
```{r P1-3 Scatter}
df_hetero <- df %>%
  add_predictions(lm_1) %>%
  add_residuals(lm_1)

ggplot(df_hetero, aes(pred, resid)) +
  geom_point(alpha = .2) +
  geom_hline(yintercept = 0, linetype = 2) +
  geom_quantile(method = "rqss", lambda = 5, quantiles = c(.05, .95)) +
  labs(title = "Homoscedastic variance of error terms",
       x = "Predicted values",
       y = "Residuals")

```
  
For detecting heteroscedasticity problem, I apply the residual plot. From the plot, we can observe that the residuals for observations with higher predicted values (> 65) and lower predicted values (< 57) have smaller variances, compared to the observations with middle levels of predicted values.  

```{r P1-3 Stat}
bptest(lm_1)

```
  
In the Breusch-Pagan test, we also have to reject the null hypothesis, which indicates the present of heteroscedasticity.

```{r P1-3 Solution}
weights <- 1 / residuals(lm_1) ^ 2

lm_wls <- lm(biden ~ age + female + educ, data=df, weights=weights)
pander(lm_wls)

```
  
To counter the problem of heteroscedasticity, I propose applying the weighted least squares regression as estimated above. In this model, we 
 assume that the errors are independent and normally distributed with mean zero and different variances. The weights are estimated from the error variances of the original model. Comparing the new model to the original one, the parameter estimates change not much. However, the standard errors become a lot smaller, which can potentially be biased. The weights, therefore, are suggested to be estimated from relevant explanatory variables (if having any theory), instead of basing only on original residuals.


### 4. Test for multicollinearity. If present, propose if/how to solve the problem.
```{r P1-4 Correlation}
cormat_heatmap <- function(data){
  # generate correlation matrix
  cormat <- round(cor(data), 2)
  
  # melt into a tidy table
  get_upper_tri <- function(cormat){
    cormat[lower.tri(cormat)]<- NA
    return(cormat)
  }
  
  upper_tri <- get_upper_tri(cormat)
  
  # reorder matrix based on coefficient value
  reorder_cormat <- function(cormat){
    # Use correlation between variables as distance
    dd <- as.dist((1-cormat)/2)
    hc <- hclust(dd)
    cormat <-cormat[hc$order, hc$order]
  }
  
  cormat <- reorder_cormat(cormat)
  upper_tri <- get_upper_tri(cormat)
  
  # Melt the correlation matrix
  melted_cormat <- reshape2::melt(upper_tri, na.rm = TRUE)
  
  # Create a ggheatmap
  ggheatmap <- ggplot(melted_cormat, aes(Var2, Var1, fill = value))+
    geom_tile(color = "white")+
    scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
                         midpoint = 0, limit = c(-1,1), space = "Lab", 
                         name="Pearson\nCorrelation") +
    theme_minimal()+ # minimal theme
    theme(axis.text.x = element_text(angle = 45, vjust = 1, 
                                     size = 12, hjust = 1))+
    coord_fixed()
  
  # add correlation values to graph
  ggheatmap + 
    geom_text(aes(Var2, Var1, label = value), color = "black", size = 4) +
    theme(
      axis.title.x = element_blank(),
      axis.title.y = element_blank(),
      panel.grid.major = element_blank(),
      panel.border = element_blank(),
      panel.background = element_blank(),
      axis.ticks = element_blank(),
      legend.position = "bottom") +
    labs(title="Correlation matrix")
}

cormat_heatmap(select(df, age, female, educ))

ggpairs(select(df, age, female, educ))

```
  
To detect collinearity, I observe the correlation matrix of explanatory variables. From the graphs, all correlations are low. There seems no high-level correlations could potentially cause collinearity problem. In the pair-wise scatter plots, no specific correlated patterns can be observed.  


```{r P1-4 Stat}
vif(lm_1)

```
  
VIF values are also computed. All are at values around 1, way smaller than 10. This indicates that we probably do not have to worry too much about the collinearity in our model.  
  
However, if the collinearity do exist, three approaches are suggested to address the problem:  
1. Add more observations to increase the probability that different levels of one variables can be observed conditional on a certain level of other variables.  
2. Transform the variables. For example, combine the variables with the collinearity problem into a new variable, which captures the information provided in all original variables while avoiding the collinearity problem.  
3. Applying Shrinkage methods, such as lasso regression, to involve all variables while shrinking all estimated coefficients toward zero. This method acquires smaller variance of the estimates with a cost of increasing the potential bias.  




## Part 2: Interaction terms
### Estimate model
```{r Estimate model P2}
lm_2 <- lm(biden ~ age + educ + age * educ, data=df)
pander(summary(lm_2))


# function to get point estimates and standard errors
# model - lm object
# mod_var - name of moderating variable in the interaction
instant_effect <- function(model, mod_var){
  # get interaction term name
  int.name <- names(model$coefficients)[[which(str_detect(names(model$coefficients), ":"))]]
  
  marg_var <- str_split(int.name, ":")[[1]][[which(str_split(int.name, ":")[[1]] != mod_var)]]
  
  # store coefficients and covariance matrix
  beta.hat <- coef(model)
  cov <- vcov(model)
  
  # possible set of values for mod_var
  if(class(model)[[1]] == "lm"){
    z <- seq(min(model$model[[mod_var]]), max(model$model[[mod_var]]))
  } else {
    z <- seq(min(model$data[[mod_var]]), max(model$data[[mod_var]]))
  }
  
  # calculate instantaneous effect
  dy.dx <- beta.hat[[marg_var]] + beta.hat[[int.name]] * z
  
  # calculate standard errors for instantaeous effect
  se.dy.dx <- sqrt(cov[marg_var, marg_var] +
                     z^2 * cov[int.name, int.name] +
                     2 * z * cov[marg_var, int.name])
  
  # combine into data frame
  data_frame(z = z,
             dy.dx = dy.dx,
             se = se.dy.dx)
}

```
  
After applying listwise missing value deletion, the above model is estimated with 1,807 observations, `age`, `female` (gender), and interaction term of `age` and `female` as predictor variables, and `biden` (Joe Biden feeling thermometer) as the outcome variable.  
  
Estimated parameters, standard errors, and significance are reported in the above table. Generally, all coefficients, including the interaction term of `age` and `female`, in this model are significant (all p-value < 0.05).  
  
Observerd by coefficient of the interaction term (-0.048), the marginal effect of age on Joe Biden thermometer rating goes down and goes negative when the education goes up, and the marginal effect of education on Joe Biden thermometer rating goes down and negative when the age goes up. Statistically, these two effects of different directions are equivelent. However, theoretically, I would stand for the first effect, the marginal effect of age on Joe Biden thermometer rating goes down when the education goes up. As people getting more education, their judgment can be more stable, and less influenced by emotions, body physical conditions, and other factors varied across people.  


### 1. Evaluate the marginal effect of age on Joe Biden thermometer rating, conditional on education. Consider the magnitude and direction of the marginal effect, as well as its statistical significance.
```{r P2-1 Graph}
#Discrete plot
instant_effect(lm_2, "educ") %>%
  ggplot(aes(z, dy.dx,
             ymin = dy.dx - 1.96 * se,
             ymax = dy.dx + 1.96 * se)) +
  geom_pointrange() +
  geom_hline(yintercept = 0, linetype = 2) +
  labs(title = "Marginal effect of age",
       subtitle = "By education",
       x = "Education",
       y = "Estimated marginal effect")

#Line plot
instant_effect(lm_2, "educ") %>%
  ggplot(aes(z, dy.dx)) +
  geom_line() +
  geom_line(aes(y = dy.dx - 1.96 * se), linetype = 2) +
  geom_line(aes(y = dy.dx + 1.96 * se), linetype = 2) +
  geom_hline(yintercept = 0) +
  labs(title = "Marginal effect of age",
       subtitle = "By education",
       x = "Education",
       y = "Estimated marginal effect")

```
  
Conditional on education, the marginal effect of age on Joe Biden thermometer rating are plotted as above. Conforming to what we observed from the estimated coefficient, the marginal effect of age goes down and eventually negative when education goes up (going from 0.65 to -0.15). From the plots, we can observe that this marginal effect is non-significant (can not tell if not different from zero) when the education is at levels between 13 and 16, while significant at other levels of education.

```{r P2-1 Stat}
linearHypothesis(lm_2, "age + 12 * age:educ")
linearHypothesis(lm_2, "age + 13 * age:educ")
linearHypothesis(lm_2, "age + 14 * age:educ")
linearHypothesis(lm_2, "age + 15 * age:educ")
linearHypothesis(lm_2, "age + 16 * age:educ")
linearHypothesis(lm_2, "age + 17 * age:educ")
linearHypothesis(lm_2, "age + 18 * age:educ")

dim(filter(df, educ >= 13 & educ <= 16))[1] / dim(df)[1]

```
  
Linear hypothesis tests at different levels of education are performed for quick examining the marginal effect of age. The results confirm our observation in the previous plots that the marginal effect is not significant when education is around 13 to 16 (p-value > 0.05), and the marginal effect is significant at other levels of education (p-value < 0.05). Around 44.6% of all observations fall in the insignificant range. In other words, it is noteworthy that the marginal effect of age is not significant for quite a large proportion of the observations as the direct effect and interaction effect counteract each other.  


### 2. Evaluate the marginal effect of education on Joe Biden thermometer rating, conditional on age. Consider the magnitude and direction of the marginal effect, as well as its statistical significance.
```{r P2-2 Graph}
#Discrete plot
instant_effect(lm_2, "age") %>%
  ggplot(aes(z, dy.dx,
             ymin = dy.dx - 1.96 * se,
             ymax = dy.dx + 1.96 * se)) +
  geom_pointrange() +
  geom_hline(yintercept = 0, linetype = 2) +
  labs(title = "Marginal effect of education",
       subtitle = "By age",
       x = "Age",
       y = "Estimated marginal effect")

#Line plot
instant_effect(lm_2, "age") %>%
  ggplot(aes(z, dy.dx)) +
  geom_line() +
  geom_line(aes(y = dy.dx - 1.96 * se), linetype = 2) +
  geom_line(aes(y = dy.dx + 1.96 * se), linetype = 2) +
  geom_hline(yintercept = 0) +
  labs(title = "Marginal effect of education",
       subtitle = "By age",
       x = "Age",
       y = "Estimated marginal effect")

```
  
Conditional on age, the marginal effect of education on Joe Biden thermometer rating are plotted as above. Conforming to what we observed from the estimated coefficient, the marginal effect of education goes down and eventually negative when education goes up (going from 1.8 to -2.8). From the plots, we cab observe that this marginal effect is non-significant (can not tell if not different from zero) when the age is at levels lower than 45 while significant at other levels of age.

```{r P2-2 Stat}
linearHypothesis(lm_2, "educ + 20 * age:educ")
linearHypothesis(lm_2, "educ + 25 * age:educ")
linearHypothesis(lm_2, "educ + 30 * age:educ")
linearHypothesis(lm_2, "educ + 35 * age:educ")
linearHypothesis(lm_2, "educ + 40 * age:educ")
linearHypothesis(lm_2, "educ + 45 * age:educ")
linearHypothesis(lm_2, "educ + 50 * age:educ")

dim(filter(df, age <= 45))[1] / dim(df)[1]

```
  
Linear hypothesis tests at different levels of education are performed for quick examining the marginal effect of education. The results confirm our observation in the previous plots that the marginal effect is significant when age is higher than 45 (p-value < 0.05) and not significant when age is below 45 (p-value > 0.05). Again, around 46.7% of all observations fall in this insignificant range. In other words, it is noteworthy that the marginal effect of education is not significant for quite a large proportion of the observations as the direct effect and interaction effect counteract each other.  



## Part 3: Missing data
```{r Setup P3}
df_mi <- read_csv("data/biden.csv") %>%
  rownames_to_column(var="row")

```

### Use multiple imputation to account for the missingness in the data. Consider the multivariate normality assumption and transform any variables as you see fit for the imputation stage.
```{r P3 Observe}
mi_all <- amelia(as.data.frame(df_mi), m=5, idvars=c("row"))

missmap(mi_all)

models_imp <- data_frame(data=mi_all$imputations) %>%
  mutate(model=map(data, ~ lm(biden ~ age + female + educ,
                                data = .x)),
         coef = map(model, tidy)) %>%
  unnest(coef, .id = "id")
models_imp

```
  
First, the multiple imputation is performed based on all variables available in the dataset. The missingness map is drawn, from which we observe that most of the missing values occur in the variable `baiden`.  

```{r P3 Variable Identification}
ggpairs(select_if(df_mi, is.numeric))

```
  
To select proper variables in estimating imputation values, a correlation matrix and pair-wise scatter plots are applied. I suggest to include `dem` and `rep` in estimating values for multiple imputation, even though they are not included in our regression model. Because the two variables are moderately correlated with `biden` (corr = 0.461 and -0.421, respectively), the variable with most missing values. In addition, all other variables in the original regression model will be included as well, including `female`, `age`, and `educ`.  
  
In the distribution plot shown above, except three binary variables, we  also observe some variables not conforming to normal distribution, such as `biden`, skewed to the left, and `age`, skewed to the right.  

```{r P3 Variable Transform}
mi_transformed <- amelia(as.data.frame(df_mi), m=5, idvars=c("row"),
                      logs=c("age"),
                      sqrts=c("biden"),
                      noms=c("dem", "rep", "female"))

```
  
To tackle the non-normal problem, I do a log transformation to `age`, and a squared-root transformation to `biden`. In addition, I mark `female`, `dem`, and `rep` as nominal variables when implementing Amelia's Multiple Imputation.  


### Calculate appropriate estimates of the parameters and the standard errors. Explain how the results differ from the original, non-imputed model.
```{r P3 Model}
models_trans_imp <- data_frame(data=mi_transformed$imputations) %>%
  mutate(model=map(data, ~ lm(biden ~ age + female + educ,
                                data = .x)),
         coef = map(model, tidy)) %>%
  unnest(coef, .id = "id")
models_trans_imp


mi.meld.plus <- function(df_tidy){
  # transform data into appropriate matrix shape
  coef.out <- df_tidy %>%
    select(id:estimate) %>%
    spread(term, estimate) %>%
    select(-id)
  
  se.out <- df_tidy %>%
    select(id, term, std.error) %>%
    spread(term, std.error) %>%
    select(-id)
  
  combined.results <- mi.meld(q = coef.out, se = se.out)
  
  data_frame(term = colnames(combined.results$q.mi),
             estimate.mi = combined.results$q.mi[1, ],
             std.error.mi = combined.results$se.mi[1, ])
}


# compare results
tidy(lm_1) %>%
  left_join(mi.meld.plus(models_trans_imp)) %>%
  select(-statistic, -p.value)

# cheating on my confidence intervals for this plot
bind_rows(orig = tidy(lm_1),
          full_imp = mi.meld.plus(models_imp) %>%
            rename(estimate = estimate.mi,
                   std.error = std.error.mi),
          trans_imp = mi.meld.plus(models_trans_imp) %>%
            rename(estimate = estimate.mi,
                   std.error = std.error.mi),
          .id = "method") %>%
  mutate(method = factor(method, levels = c("orig", "full_imp", "trans_imp"),
                         labels = c("Listwise deletion", "Full imputation",
                                    "Transformed imputation")),
         term = factor(term, levels = c("(Intercept)", "age", "female", "educ"),
                       labels = c("(Intercept)", "Age", "Female", "Education"))) %>%
  ggplot(aes(fct_rev(term), estimate, color = fct_rev(method),
             ymin = estimate - 1.96 * std.error,
             ymax = estimate + 1.96 * std.error)) +
  geom_hline(yintercept = 0, linetype = 2) +
  geom_pointrange(position = position_dodge(.75)) +
  coord_flip() +
  scale_color_discrete(guide = guide_legend(reverse = TRUE)) +
  labs(title = "Comparing regression results",
       x = NULL,
       y = "Estimated parameter",
       color = NULL) +
  theme(legend.position = "bottom")


#Remove intercept
bind_rows(orig = tidy(lm_1),
          full_imp = mi.meld.plus(models_imp) %>%
            rename(estimate = estimate.mi,
                   std.error = std.error.mi),
          trans_imp = mi.meld.plus(models_trans_imp) %>%
            rename(estimate = estimate.mi,
                   std.error = std.error.mi),
          .id = "method") %>%
  mutate(method = factor(method, levels = c("orig", "full_imp", "trans_imp"),
                         labels = c("Listwise deletion", "Full imputation",
                                    "Transformed imputation")),
         term = factor(term, levels = c("(Intercept)", "age", "female", "educ"),
                       labels = c("(Intercept)", "Age", "Female", "Education"))) %>%
  filter(term != "(Intercept)") %>%
  ggplot(aes(fct_rev(term), estimate, color = fct_rev(method),
             ymin = estimate - 1.96 * std.error,
             ymax = estimate + 1.96 * std.error)) +
  geom_hline(yintercept = 0, linetype = 2) +
  geom_pointrange(position = position_dodge(.75)) +
  coord_flip() +
  scale_color_discrete(guide = guide_legend(reverse = TRUE)) +
  labs(title = "Comparing regression results",
       x = NULL,
       y = "Estimated parameter",
       color = NULL) +
  theme(legend.position = "bottom")

```
  
The coefficient estimates (estimate.mi) and standard errors (std.error.mi) from the imputed data are presented as above.  
  
The estimated coefficients are visualized and compared in the above graphs. As observed, when compared to the original, non-imputed model, absolute value of the intercept goes down a little bit; with a higher standard error, the intercept is a little bit less significant to be different from 0. Absolute value of the parameter for `age` is slightly larger; with an slightly lower standard error, the parameter is a bit more significant to be different from 0. Absolute value of the parameter for `female` goes down around 10%; with a slightly higher standard error, the parameter is slightly less significant to be different from 0. Finally, absolute value of the parameter for `educ` goes down 20%; as the standard error increases, the parameter is less significant to be different from 0.  
